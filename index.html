<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title>Interactive Graphics  -  Final Project</title>
		<link rel="stylesheet" href="style/style.css">

        <meta name="description"   content="Interactive Graphics  -  Final Project - 2019">
        <meta name="keywords"      content="HTML, CSS, JavaScript, GLSL">
        <meta name="author"        content="JohnnyMDA">        
        <meta name="viewport"      content="width=device-width, initial-scale=1.0">
        <link rel="shortcut icon"  href="ig_cube.ico">

        <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,400i,700|PT+Serif:400,400i&subset=latin-ext" rel="stylesheet">
		<link rel="stylesheet" type="text/css" href="style/style.css">

		<script src="Common/jquery-3.4.0.min.js"></script>
		<script src="Common/general_purpose_scripts.js"></script>

		<script src="Common/dsp.js"></script>		<!-- signal FTT (Fast Fourier Transform) -->
		<script src="Common/three.min.js"></script>
		<script src="Common/Stats.js"></script>		<!-- JavaScript Performance Monitor -->
		
	</head>
	
	<body>
	
		<div id="animate"> 	</div>

		<div align="center">
			<audio id="my_audio" src="audio/Me & My Toothbrush - Push the Tempo.mp3" autobuffer autoloop loop controls>Your browser does not support the audio element.</audio>
		</div>

		<script>
			// set variables
			var audio,
			    frameBufferLength,
			    bufferSize,
			    signal,
			    peak,
			    fft;

			// Initialize the visualization
			window.onload=function()
			{
				init();
			}

			function init()
			{
				// Get the audio element
				audio = document.getElementById("my_audio");

				// Audio event listeners
				if(true)
				{
					audio.addEventListener("loadeddata", 		onLoadedMetadata);
					audio.addEventListener("MozAudioAvailable", onAudioAvailable);
				}


				// Run when metadata has loaded on the audio to be played
				function onLoadedMetadata()
				{
					console.log("<<< start --- onLoadedMetadata() >>>");
					// Get the  # of channels  of the audio (1-mono, 2-stereo)
					channels = audio.mozChannels;

					// Set up the frame buffer length
					frameBufferLength = audio.mozFrameBufferLength;

					// We will use mono for dsp.js, so the buffer size is given by:
					bufferSize = frameBufferLength / channels;

					// Set up dsp.js for audio analysis (ready to use)
					signal 	= new Float32Array(bufferSize);
					peak 	= new Float32Array(bufferSize);
					fft 	= new FFT(bufferSize, 44100);
					console.log("fft = " + fft.spectrum);
				};


				// Run when audio data is available
				function onAudioAvailable(event)
				{
					// event.frameBuffer  is a Float32Array with the raw audio data (32-bit float values) obtained from decoding the audio;
					console.log("Audio available, processing...");

					// Audio FFT visualization from dsp.js;
					// Deinterleave and mix down to <mono> channel audio;
					signal = DSP.getChannel(DSP.MIX, event.frameBuffer);
					
					// Perform forward transform of the signal
					fft.forward(signal);

					// Compute and get the peak values of the signal
					for (var i=0; i < bufferSize; i++)
					{
						// Equalizer: attenuate low frequents and boost the high ones
						fft.spectrum[i] *= -1 * Math.log((fft.bufferSize / 2 - i) * (0.5 / fft.bufferSize / 2)) * fft.bufferSize;

						if (peaak[i] < fft.spectrum[i])
						{
							// Assign a new peak value
							peak[i] = fft.spectrum[i];
						} 
						else 
						{
							// decreasing the peak slowly till a new peak is found
							peak[i] *= 0.987;
						}
					}
				}; // --- end onAudioAvailable(event) --- //

				

			}; // --- end init() --- //
		</script>


	</body>
</html>